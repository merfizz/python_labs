import re

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    """–§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –µ–¥–∏–Ω–æ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Å—Ç–∏–ª—é"""
    s=text
    if casefold :
        s=s.casefold()
    if yo2e :
        s=s.replace("—ë","–µ")
        s=s.replace("–Å","–ï")
    s=s.replace("\t"," ")
    s=s.replace("\r"," ")
    s=s.replace("\n"," ")
    s = ' '.join(s.split())
    s=s.strip()

    return s

def tokenize(text: str) -> list[str]:
    """–§—É–Ω–∫—Ü–∏—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ ¬´—Å–ª–æ–≤–∞¬ª –ø–æ –Ω–µ–±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º"""
    pattern = r'[\w-]+'
    tokenstext = re.findall(pattern, text)

    return tokenstext

def count_freq(tokens: list[str]) -> dict[str, int]:
    """–§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤–æ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"""
    counts={}
    for word in tokens:
        counts[word]=counts.get(word,0)+1
    return counts

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    """–§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã"""
    sorted_freq= sorted(freq.items(),key=lambda item: [-item[1], item[0]])
    top_n=[]

    for i in range(min(n, len(sorted_freq))):
        top_n.append((sorted_freq[i][0], sorted_freq[i][1]))

    return top_n

print( normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print( normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print( normalize("Hello\r\nWorld"))
print( normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))

print( tokenize(normalize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")))
print( tokenize(normalize("hello,world!!!")))
print( tokenize(normalize("2025 –≥–æ–¥")))
print( tokenize(normalize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ")))

print (count_freq(["a","b","a","c","b","a"]),top_n(count_freq(["a","b","a","c","b","a"]),2))
print (count_freq(["bb","aa","bb","aa","cc"]),top_n(count_freq(["bb","aa","bb","aa","cc"]),2))


